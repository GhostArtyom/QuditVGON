{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "from typing import List\n",
    "from Hamiltonian import *\n",
    "from qudit_mapping import *\n",
    "from qutrit_synthesis import *\n",
    "from VAE_model import VAEModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations\n",
    "import torch.distributions as dists\n",
    "from exact_diagonalization import *\n",
    "from scipy.linalg import norm, orth\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.sparse import csr_matrix, eye\n",
    "\n",
    "np.set_printoptions(precision=8, linewidth=200)\n",
    "torch.set_printoptions(precision=8, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "energy, fidelity, cos_sim_max, cos_sim_mean, cos_sim_min = [], [], [], [], []\n",
    "with open('./logs/VGON_nqd7_L3_phase_202501.log') as f:\n",
    "    lines, count = f.readlines(), 0\n",
    "    for line in lines:\n",
    "        energy_pattern = r'Energy: (-*\\d\\.\\d+)'\n",
    "        fidelity_pattern = r'Fidelity: (\\d\\.\\d+)'\n",
    "        cos_sim_pattern = r'Cos_Sim: \\d+\\*(\\d\\.\\d+), (\\d\\.\\d+), (-*\\d\\.\\d+)'\n",
    "        energy_match = re.search(energy_pattern, line)\n",
    "        fidelity_match = re.search(fidelity_pattern, line)\n",
    "        cos_sim_match = re.search(cos_sim_pattern, line)\n",
    "        if energy_match and fidelity_match and cos_sim_match:\n",
    "            energy.append(float(energy_match.group(1)))\n",
    "            fidelity.append(float(fidelity_match.group(1)))\n",
    "            cos_sim_max.append(float(cos_sim_match.group(1)))\n",
    "            cos_sim_mean.append(float(cos_sim_match.group(2)))\n",
    "            cos_sim_min.append(float(cos_sim_match.group(3)))\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "x = 1 + np.arange(len(energy))\n",
    "axs[0].plot(x, energy, linewidth=1)\n",
    "axs[0].set_title('Energy', fontsize=12)\n",
    "axs[0].axhline(-8.85166577, color='r', linestyle='--')\n",
    "axs[1].plot(x, fidelity, linewidth=1)\n",
    "axs[1].set_title('Fidelity', fontsize=12)\n",
    "if max(fidelity) > 0.9:\n",
    "    axs[1].axhline(1, color='r', linestyle='--')\n",
    "    axs[1].axhline(0.95, color='g', linestyle='--')\n",
    "    axs[1].set_ylim(-0.05, 1.05)\n",
    "    axs[1].set_yticks(np.linspace(0, 1, 11))\n",
    "plt.show()\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "x = 1 + np.arange(len(cos_sim_max))\n",
    "axs[0].plot(x, cos_sim_max, linewidth=0.5)\n",
    "axs[0].set_title('Cos Sim Max', fontsize=12)\n",
    "axs[1].plot(x, cos_sim_mean, linewidth=0.5)\n",
    "axs[1].set_title('Cos Sim Mean', fontsize=12)\n",
    "axs[2].plot(x, cos_sim_min, linewidth=0.5)\n",
    "axs[2].set_title('Cos Sim Min', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "\n",
    "path = './mats'\n",
    "date = '20250103'\n",
    "for name in sorted(os.listdir(path), reverse=True):\n",
    "    match = re.search(r'(VGON+)_nqd\\d+(_L\\d+)*_\\d{8}_\\d{6}.mat', name)\n",
    "    if match and date in name:\n",
    "        task = match.group(1)\n",
    "        load = loadmat(f'{path}/{name}')\n",
    "        phase = load['phase'].item()\n",
    "        energy = load['energy'].item()\n",
    "        fidelity = load['fidelity'].sum()\n",
    "        n_train = load['n_train'].item()\n",
    "        ground_state_energy = load['ground_state_energy'].item()\n",
    "        energy_gap = energy - ground_state_energy\n",
    "        print(f'{name}, {phase}, {ground_state_energy:.8f}, {energy:.8f}, {energy_gap:.4e}, {fidelity.sum():.8f}, {n_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from utils import fidelity\n",
    "from scipy.io import loadmat\n",
    "from VAE_model import VAEModel\n",
    "from Hamiltonian import BBH_model\n",
    "from itertools import combinations\n",
    "import torch.distributions as dists\n",
    "from torch.utils.data import DataLoader\n",
    "from qudit_mapping import symmetric_decoding\n",
    "from qutrit_synthesis import NUM_PR, two_qutrit_unitary_synthesis\n",
    "\n",
    "np.set_printoptions(precision=8, linewidth=200)\n",
    "torch.set_printoptions(precision=8, linewidth=200)\n",
    "\n",
    "n_test = 10\n",
    "name = 'VGON_nqd7_L3_20250103_121746'\n",
    "\n",
    "if True:\n",
    "    load = loadmat(f'./mats/{name}.mat')\n",
    "    theta = load['theta'].item()\n",
    "    phase = load['phase'].item()\n",
    "    kl_div = load['kl_div'].item()\n",
    "    energy = load['energy'].item()\n",
    "    n_train = load['n_train'].item()\n",
    "    n_layers = load['n_layers'].item()\n",
    "    n_qudits = load['n_qudits'].item()\n",
    "    batch_size = load['batch_size'].item()\n",
    "    ground_states = load['ground_states']\n",
    "    fidelity_mean = load['fidelity'].squeeze()\n",
    "    ground_state_energy = load['ground_state_energy'].item()\n",
    "    degeneracy = ground_states.shape[0]\n",
    "\n",
    "    print(f'{name}, {phase}, Fidelity: {fidelity_mean.max():.8f}, KL_div: {kl_div:.4e}, {degeneracy}')\n",
    "    print(f'Ground State Energy: {ground_state_energy:.8f}, Energy: {energy:.8f}, Gap: {energy-ground_state_energy:.4e}, {n_train}')\n",
    "\n",
    "    n_qubits = 2 * n_qudits\n",
    "    n_samples = batch_size * n_test\n",
    "    n_params = n_layers * (n_qudits - 1) * NUM_PR\n",
    "\n",
    "    z_dim = 50\n",
    "    list_z = np.arange(np.floor(np.log2(n_params)), np.ceil(np.log2(z_dim)) - 1, -1)\n",
    "    h_dim = np.power(2, list_z).astype(int)\n",
    "\n",
    "    dev = qml.device('default.qubit', n_qubits)\n",
    "    gpu_memory = gpus[0].memoryUtil if (gpus := GPUtil.getGPUs()) else 1\n",
    "    if torch.cuda.is_available() and gpu_memory < 0.8 and n_qubits >= 12:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    def qutrit_symmetric_ansatz(params: torch.Tensor):\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj = list(range(n_qubits - 2 * i - 4, n_qubits - 2 * i))\n",
    "            two_qutrit_unitary_synthesis(params[i], obj)\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_state(n_layers: int, params: torch.Tensor):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.state()\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_expval(n_layers: int, params: torch.Tensor, Ham):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.expval(Ham)\n",
    "\n",
    "    state_dict = torch.load(f'./mats/{name}.pt', map_location=device, weights_only=True)\n",
    "    model = VAEModel(n_params, z_dim, h_dim).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    data_dist = dists.Uniform(0, 1).sample([n_samples, n_params])\n",
    "    test_data = DataLoader(data_dist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    qubit_Ham = BBH_model(n_qudits, theta)\n",
    "    overlaps = np.empty((0, ground_states.shape[0]))\n",
    "    for i, batch in enumerate(test_data):\n",
    "        with torch.no_grad():\n",
    "            params, _, _ = model(batch.to(device))\n",
    "            energy = circuit_expval(n_layers, params, qubit_Ham)\n",
    "            energy_mean = energy.mean()\n",
    "            energy_gap = energy_mean - ground_state_energy\n",
    "\n",
    "            states = circuit_state(n_layers, params).detach().cpu().numpy()\n",
    "            for state in states:\n",
    "                decoded_state = symmetric_decoding(state, n_qudits)\n",
    "                overlap = np.array([fidelity(decoded_state, ground_state) for ground_state in ground_states])\n",
    "                overlaps = np.vstack((overlaps, overlap))\n",
    "\n",
    "            fidelities = np.array([fidelity(states[ind[0]], states[ind[1]]) for ind in combinations(range(batch_size), 2)])\n",
    "            print(f'Energy: {energy_mean:.8f}, {energy_gap:.4e}, Fidelity: {fidelities.max():.8f}, {fidelities.min():.8f}, Overlap: {overlap}, {i+1}/{n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ptitprince as pt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_raincloud(overlaps: np.ndarray):\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    data = {r'$|\\varphi_{\\!' + f'{i+1}' + r'}\\rangle$': overlaps[:, i] for i in range(num_phi)}\n",
    "    df = pd.DataFrame(data)\n",
    "    pt.half_violinplot(data=df, scale='count', width=0.8, inner=None, linewidth=1.2, palette=colors)\n",
    "    sns.stripplot(data=df, jitter=True, palette=colors, size=1.5)\n",
    "    plt.ylabel('Overlaps', labelpad=10, fontsize=16)\n",
    "    # plt.yticks(np.linspace(0, 1, 11))\n",
    "    plt.xlim(-0.8, 3.4)\n",
    "\n",
    "\n",
    "num_phi = overlaps.shape[1]\n",
    "colors = ['#74B816', '#339AF0', '#FD7E14', '#F03E3E']\n",
    "plot_raincloud(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.array([-0.74, -0.26, -0.24, 0.24, 0.26, 0.49]) * np.pi\n",
    "coeffs = np.append(coeffs, np.arctan(1 / 3))\n",
    "for theta in coeffs:\n",
    "    if theta == np.arctan(1 / 3):\n",
    "        phase = 'arctan(1/3)'\n",
    "    else:\n",
    "        phase = f'{theta/np.pi:.2f}π'\n",
    "    if np.pi / 4 < theta and theta < np.pi / 2:\n",
    "        print(f'Critical: {phase}')\n",
    "    elif -3 * np.pi / 4 < theta and theta < -np.pi / 4:\n",
    "        print(f'Dimerized: {phase}')\n",
    "    elif -np.pi / 4 < theta and theta < np.pi / 4:\n",
    "        print(f'Haldane: {phase}')\n",
    "    elif (np.pi / 2 < theta and theta < np.pi) or (-np.pi < theta and theta < -3 * np.pi / 4):\n",
    "        print(f'Ferromagnetic: {phase}')\n",
    "    else:\n",
    "        print(f'Wrong: {theta:8f}, {phase}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigensolver(n_qudits: int, k: int, theta: float):\n",
    "    n_qubits = 2 * n_qudits\n",
    "    h3 = qutrit_BBH_model(n_qudits, theta)\n",
    "    h2 = qubit_BBH_model(n_qudits, theta)\n",
    "    v3 = np.sort(eigsh(h3, k, which='SA', return_eigenvectors=False))\n",
    "    v2 = np.sort(eigsh(h2, k, which='SA', return_eigenvectors=False))\n",
    "    print(f'nqd: {n_qudits:2d} {v3}')\n",
    "    print(f'nqb: {n_qubits:2d} {v2}')\n",
    "\n",
    "\n",
    "n_qudits, k = 7, 6\n",
    "for theta in np.array([-0.74, -0.26, -0.24, 0.24, 0.26, 0.49]) * np.pi:\n",
    "    print(f'Coefficient phase: {theta/np.pi:.2f}π')\n",
    "    eigensolver(n_qudits, k, theta)\n",
    "print(f'Coefficient phase: arctan(1/3)')\n",
    "eigensolver(n_qudits, k, theta=np.arctan(1 / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from exact_diagonalization import *\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "\n",
    "def eigensolver(n_qudits: int, k: int, theta: float):\n",
    "    n_qubits = 2 * n_qudits\n",
    "    h3 = qutrit_BBH_model(n_qudits, theta, is_csr=False)\n",
    "    h2 = qubit_BBH_model(n_qudits, theta, is_csr=False)\n",
    "    v3 = np.sort(np.linalg.eigvalsh(h3))\n",
    "    v2 = np.sort(np.linalg.eigvalsh(h2))\n",
    "    print(f'nqd: {n_qudits:2d}, {v3[:k]}')\n",
    "    print(f'nqb: {n_qubits:2d}, {v2[:k]}')\n",
    "\n",
    "\n",
    "n_qudits, k = 6, 8\n",
    "for theta in np.array([0.32, -0.71, -0.30, -0.16]) * np.pi:\n",
    "    print(f'Coefficient phase: {theta/np.pi:.2f}π')\n",
    "    eigensolver(n_qudits, k, theta)\n",
    "print(f'Coefficient phase: arctan(1/3)')\n",
    "eigensolver(n_qudits, k, theta=np.arctan(1 / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "num, date = 0, '2024112'\n",
    "threshold = np.array([0, 0, 0])\n",
    "for name in sorted(os.listdir('./mats'), reverse=True):\n",
    "    match = re.search(r'(VGON_nqd\\d+_\\d{8}_\\d{6}).mat', name)\n",
    "    if match and date in name:\n",
    "        load = loadmat(f'./mats/{name}')\n",
    "        energy = load['energy'].item()\n",
    "        n_train = load['n_train'].item()\n",
    "        fidelity_max = load['fidelity_max'].item()\n",
    "        if 'count' in load:\n",
    "            count = re.search(r'(\\d+)/(\\d+)', load['count'].item())\n",
    "            energy_th, total = int(count.group(1)), int(count.group(2))\n",
    "            if energy_th > 1000 and energy < -3.99 and fidelity_max < 0.98:\n",
    "                num += 1\n",
    "                overlaps = load['overlaps']\n",
    "                overlap_sum = overlaps.sum(axis=1)\n",
    "                overlap_th = len(overlap_sum[overlap_sum > 0.995])\n",
    "\n",
    "                threshold += np.array([overlap_th, energy_th, total])\n",
    "                energy_str = f'Energy: {energy_th}/{total} = {energy_th/total*100:.2f}%'\n",
    "                overlap_str = f'Overlaps: {overlap_th}/{total} = {overlap_th/total*100:.2f}%'\n",
    "                print(f'{num:2d}, {name}, {n_train}, {energy_str}, {overlap_str}')\n",
    "\n",
    "overlap_th, energy_th, total = threshold\n",
    "print(f'Energy Threshold: {energy_th}/{total} = {energy_th/total*100:.1f}% {energy_th/total:.8f}')\n",
    "print(f'Overlaps Threshold: {overlap_th}/{total} = {overlap_th/total*100:.1f}% {overlap_th/total:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "date = '20241210'\n",
    "threshold = np.array([0, 0, 0])\n",
    "with open('./logs/VGON_nqd7_generating_202412.log') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'Load' in line:\n",
    "            match = re.search(r'Load:.+(VGON_nqd7_' + date + r'_\\d{6})\\.mat, (\\d+/\\d+)', line)\n",
    "        if 'Energy Upper' in line and match:\n",
    "            energy_upper = float(re.search(r'Energy Upper: (-\\d+\\.\\d+)', line).group(1))\n",
    "        if '100/100' in line and match:\n",
    "            name = match.group(1)\n",
    "            n_train = match.group(2)\n",
    "            load = loadmat(f'./mats/{name}.mat')\n",
    "            overlaps = load['overlaps']\n",
    "            overlap_sum = overlaps.sum(axis=1)\n",
    "            overlap_th = len(overlap_sum[overlap_sum > 0.995])\n",
    "            count = re.search(r'(\\d+)/(\\d+), 100/100', line)\n",
    "            energy_th, total = int(count.group(1)), int(count.group(2))\n",
    "            if energy_th > 1000 and energy_upper == -3.99:\n",
    "                threshold += np.array([overlap_th, energy_th, total])\n",
    "                energy_str = f'Energy: {energy_th}/{total} = {energy_th/total*100:.2f}%'\n",
    "                overlap_str = f'Overlaps: {overlap_th}/{total} = {overlap_th/total*100:.2f}%'\n",
    "                print(f'{name}, {n_train}, {energy_str}, {overlap_str}')\n",
    "overlap_th, energy_th, total = threshold\n",
    "print(f'Energy Threshold: {energy_th}/{total} = {energy_th/total*100:.1f}% {energy_th/total:.8f}')\n",
    "print(f'Overlaps Threshold: {overlap_th}/{total} = {overlap_th/total*100:.1f}% {overlap_th/total:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('./logs/VGON_nqd7_degeneracy_202412.log') as f:\n",
    "    count = 0\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        energy_pattern = r'Energy: (-*\\d\\.\\d+)'\n",
    "        fidelity_pattern = r'Fidelity: (\\d\\.\\d+)'\n",
    "        energy_match = re.search(energy_pattern, line)\n",
    "        fidelity_match = re.search(fidelity_pattern, line)\n",
    "        if energy_match and fidelity_match:\n",
    "            energy = float(energy_match.group(1))\n",
    "            fidelity = float(fidelity_match.group(1))\n",
    "            if energy < -3.99 and fidelity < 0.98:\n",
    "                count += 1\n",
    "                print(count, line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def spin_operator(obj: List[int]):\n",
    "    if len(obj) != 2:\n",
    "        raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "    sx = qml.X(obj[0]) + qml.X(obj[1])\n",
    "    sy = qml.Y(obj[0]) + qml.Y(obj[1])\n",
    "    sz = qml.Z(obj[0]) + qml.Z(obj[1])\n",
    "    return sx, sy, sz\n",
    "\n",
    "\n",
    "def spin_operator2(obj: List[int]):\n",
    "    if len(obj) != 2:\n",
    "        raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "    s1 = spin_operator(obj)\n",
    "    s2 = [i @ j for i in s1 for j in s1]\n",
    "    return s2\n",
    "\n",
    "\n",
    "def Hamiltonian(n_qudits: int, beta: float):\n",
    "    ham1, ham2 = 0, 0\n",
    "    for i in range(n_qudits - 1):\n",
    "        obj1 = [2 * i, 2 * i + 1]\n",
    "        obj2 = [2 * i + 2, 2 * i + 3]\n",
    "        ham1 += qml.sum(*[spin_operator(obj1)[i] @ spin_operator(obj2)[i] for i in range(3)])\n",
    "        ham2 += qml.sum(*[spin_operator2(obj1)[i] @ spin_operator2(obj2)[i] for i in range(9)])\n",
    "    ham = ham1 / 4 - beta * ham2 / 16\n",
    "    coeffs, obs = qml.simplify(ham).terms()\n",
    "    coeffs = torch.tensor(coeffs).real\n",
    "    return qml.Hamiltonian(coeffs, obs)\n",
    "\n",
    "\n",
    "Hamiltonian(2, -1 / 3) * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.arange(0, 1, 1e-3), []\n",
    "for cos_sim_max in x:\n",
    "    if cos_sim_max > 0.9:\n",
    "        cos_sim_max_coeff = 8\n",
    "    elif cos_sim_max > 0.8:\n",
    "        cos_sim_max_coeff = 4\n",
    "    elif cos_sim_max > 0.7:\n",
    "        cos_sim_max_coeff = 2\n",
    "    else:\n",
    "        cos_sim_max_coeff = 1\n",
    "    y.append(cos_sim_max_coeff)\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "plt.plot(x, y)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 8\n",
    "cos_sim_lower = 0.0\n",
    "coeff_funcs = [\n",
    "    lambda cos_sim_max: coeff * cos_sim_max,\n",
    "    lambda cos_sim_max: (coeff * cos_sim_max).ceil(),\n",
    "    lambda cos_sim_max: 2 * (0.5 * coeff * cos_sim_max).ceil(),\n",
    "    lambda cos_sim_max: 0.5 * (2 * coeff * cos_sim_max).ceil(),\n",
    "]\n",
    "cos_sim_max = torch.arange(cos_sim_lower, 1, 1e-3)\n",
    "for coeff_func in coeff_funcs:\n",
    "    plt.plot(cos_sim_max, coeff_func(cos_sim_max))\n",
    "plt.xticks(np.linspace(cos_sim_lower, 1, 11))\n",
    "plt.yticks(np.linspace(coeff * cos_sim_lower, coeff, 9))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 40\n",
    "cos_sim_lower = 0.5\n",
    "coeff_funcs = [\n",
    "    lambda cos_sim_max: coeff * (cos_sim_max - cos_sim_lower),\n",
    "    lambda cos_sim_max: (coeff * (cos_sim_max - cos_sim_lower)).ceil(),\n",
    "    lambda cos_sim_max: 0.5 * (2 * coeff * (cos_sim_max - cos_sim_lower)).ceil(),\n",
    "    lambda cos_sim_max: coeff * (cos_sim_max - cos_sim_lower) * cos_sim_max,\n",
    "]\n",
    "cos_sim_max = torch.arange(cos_sim_lower, 1, 1e-3)\n",
    "for coeff_func in coeff_funcs:\n",
    "    plt.plot(cos_sim_max, coeff_func(cos_sim_max))\n",
    "plt.xticks(np.linspace(cos_sim_lower, 1, 11))\n",
    "plt.yticks(np.linspace(0, coeff * (1 - cos_sim_lower), 11))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 8\n",
    "coeff_funcs = [\n",
    "    lambda cos_sim: coeff * cos_sim,\n",
    "    lambda cos_sim: coeff * cos_sim.pow(2),\n",
    "    lambda cos_sim: coeff * (2 * cos_sim - cos_sim.pow(2)),\n",
    "    lambda cos_sim: coeff / 10 * (10 * cos_sim).ceil(),\n",
    "]\n",
    "cos_sim = torch.arange(0, 1, 1e-3)\n",
    "for coeff_func in coeff_funcs:\n",
    "    plt.plot(cos_sim, torch.where(cos_sim > 0, coeff_func(cos_sim), 0))\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "plt.yticks(np.linspace(0, coeff, 11))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from torch import Tensor\n",
    "from VAE_model import VAEModel\n",
    "from typing import List, Optional\n",
    "from itertools import combinations\n",
    "import torch.distributions as dists\n",
    "from torch.utils.data import DataLoader\n",
    "from qutrit_synthesis import NUM_PR, two_qutrit_unitary_synthesis\n",
    "from torch.optim.optimizer import Optimizer, _use_grad_for_differentiable\n",
    "\n",
    "np.set_printoptions(precision=8, linewidth=200)\n",
    "torch.set_printoptions(precision=8, linewidth=200)\n",
    "\n",
    "opt = 'OGD'\n",
    "n_test = 1\n",
    "n_qudits = 2\n",
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "learning_rate = 5e-2\n",
    "\n",
    "\n",
    "class OGD(Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        momentum: float = 0,\n",
    "        weight_decay: float = 0,\n",
    "        maximize: bool = False,\n",
    "        differentiable: bool = False,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f'Invalid learning rate: {lr}')\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f'Invalid momentum value: {momentum}')\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f'Invalid weight_decay value: {weight_decay}')\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay,\n",
    "            maximize=maximize,\n",
    "            differentiable=differentiable,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def _init_group(self, group, params, grads, momentum_buffer):\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:\n",
    "                params.append(p)\n",
    "                grads.append(p.grad)\n",
    "                if group['momentum'] != 0:\n",
    "                    state = self.state[p]\n",
    "                    momentum_buffer.append(state.get('momentum_buffer'))\n",
    "\n",
    "    @_use_grad_for_differentiable\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            params: List[Tensor] = []\n",
    "            grads: List[Tensor] = []\n",
    "            momentum_buffer: List[Optional[Tensor]] = []\n",
    "            self._init_group(group, params, grads, momentum_buffer)\n",
    "            orthogonal_gradient_descent(\n",
    "                params,\n",
    "                grads,\n",
    "                momentum_buffer,\n",
    "                lr=group['lr'],\n",
    "                momentum=group['momentum'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                maximize=group['maximize'],\n",
    "            )\n",
    "            if group['momentum'] != 0:\n",
    "                # update momentum_buffers in state\n",
    "                for p, buff in zip(params, momentum_buffer):\n",
    "                    state = self.state[p]\n",
    "                    state['momentum_buffer'] = buff\n",
    "\n",
    "\n",
    "def orthogonal_gradient_descent(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    momentum_buffer: List[Optional[Tensor]],\n",
    "    lr: float,\n",
    "    momentum: float,\n",
    "    weight_decay: float,\n",
    "    maximize: bool,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i] if not maximize else -grads[i]\n",
    "        if weight_decay != 0:\n",
    "            grad.add_(param, alpha=weight_decay)\n",
    "            # grad = grad.add(param, alpha=weight_decay)\n",
    "        if momentum != 0:\n",
    "            buffer = momentum_buffer[i]\n",
    "            if buffer is None:\n",
    "                buffer = torch.clone(grad).detach()\n",
    "                momentum_buffer[i] = buffer\n",
    "            else:\n",
    "                buffer.mul_(momentum).add_(grad, alpha=1)\n",
    "            grad = buffer\n",
    "        param.add_(grad, alpha=-lr)\n",
    "    print(param)\n",
    "    print(grad)\n",
    "\n",
    "\n",
    "if True:\n",
    "    n_layers = 2\n",
    "    beta = -1 / 3\n",
    "    batch_size = 16\n",
    "    energy_coeff, kl_coeff = 1, 1\n",
    "    energy_tol, kl_tol = 1e-2, 1e-5\n",
    "\n",
    "    n_qubits = 2 * n_qudits\n",
    "    n_samples = batch_size * n_test\n",
    "    n_params = n_layers * (n_qudits - 1) * NUM_PR\n",
    "    ground_state_energy = -2 / 3 * (n_qudits - 1)\n",
    "    print(f'Ground State Energy: {ground_state_energy:.6f}')\n",
    "\n",
    "    z_dim = 50\n",
    "    list_z = np.arange(np.floor(np.log2(n_params)), np.ceil(np.log2(z_dim)) - 1, -1)\n",
    "    h_dim = np.power(2, list_z).astype(int)\n",
    "\n",
    "    dev = qml.device('default.qubit', n_qubits)\n",
    "    gpu_memory = gpus[0].memoryUtil if (gpus := GPUtil.getGPUs()) else 1\n",
    "    if torch.cuda.is_available() and gpu_memory < 0.8 and n_qubits >= 12:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    def spin_operator(obj: List[int]):\n",
    "        if len(obj) != 2:\n",
    "            raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "        sx = qml.X(obj[0]) + qml.X(obj[1])\n",
    "        sy = qml.Y(obj[0]) + qml.Y(obj[1])\n",
    "        sz = qml.Z(obj[0]) + qml.Z(obj[1])\n",
    "        return sx, sy, sz\n",
    "\n",
    "    def spin_operator2(obj: List[int]):\n",
    "        if len(obj) != 2:\n",
    "            raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "        s1 = spin_operator(obj)\n",
    "        s2 = [i @ j for i in s1 for j in s1]\n",
    "        return s2\n",
    "\n",
    "    def Hamiltonian(n_qudits: int, beta: float):\n",
    "        ham1, ham2 = 0, 0\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj1 = [2 * i, 2 * i + 1]\n",
    "            obj2 = [2 * i + 2, 2 * i + 3]\n",
    "            ham1 += qml.sum(*[spin_operator(obj1)[i] @ spin_operator(obj2)[i] for i in range(3)])\n",
    "            ham2 += qml.sum(*[spin_operator2(obj1)[i] @ spin_operator2(obj2)[i] for i in range(9)])\n",
    "        ham = ham1 / 4 - beta * ham2 / 16\n",
    "        coeffs, obs = qml.simplify(ham).terms()\n",
    "        coeffs = torch.tensor(coeffs).real\n",
    "        return qml.Hamiltonian(coeffs, obs)\n",
    "\n",
    "    def qutrit_symmetric_ansatz(params: torch.Tensor):\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj = list(range(n_qubits - 2 * i - 4, n_qubits - 2 * i))\n",
    "            two_qutrit_unitary_synthesis(params[i], obj)\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_state(n_layers: int, params: torch.Tensor):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.state()\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_expval(n_layers: int, params: torch.Tensor, Ham):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.expval(Ham)\n",
    "\n",
    "    Ham = Hamiltonian(n_qudits, beta)\n",
    "    model = VAEModel(n_params, z_dim, h_dim).to(device)\n",
    "\n",
    "if opt == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "elif opt == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "elif opt == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "else:\n",
    "    optimizer = OGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data_dist = dists.Uniform(0, 1).sample([n_samples, n_params])\n",
    "train_data = DataLoader(data_dist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i, batch in enumerate(train_data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    params, mean, log_var = model(batch.to(device))\n",
    "\n",
    "    energy = circuit_expval(n_layers, params, Ham)\n",
    "    energy_mean = energy.mean()\n",
    "\n",
    "    kl_div = -0.5 * (1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    kl_div = kl_div.mean()\n",
    "\n",
    "    cos_sims = torch.empty((0), device=device)\n",
    "    for ind in combinations(range(batch_size), 2):\n",
    "        cos_sim = torch.cosine_similarity(params[ind[0], :], params[ind[1], :], dim=0)\n",
    "        cos_sims = torch.cat((cos_sims, cos_sim.unsqueeze(0)), dim=0)\n",
    "    cos_sim_max = cos_sims.max()\n",
    "    cos_sim_mean = cos_sims.mean()\n",
    "\n",
    "    coeff = (energy_mean - ground_state_energy).ceil()\n",
    "    cos_sim_max_coeff = (1 * cos_sim_max).ceil()\n",
    "    cos_sim_mean_coeff = coeff / 10 * (10 * cos_sim_mean).ceil() if cos_sim_mean > 0 else 0\n",
    "    loss = energy_coeff * energy_mean + kl_coeff * kl_div + cos_sim_max_coeff * cos_sim_max\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    t = time.perf_counter() - start\n",
    "    cos_sim_str = f'Cos_Sim: {cos_sim_max_coeff:.0f}*{cos_sim_max:.6f}, {cos_sim_mean:.6f}'\n",
    "    print(f'Loss: {loss:.8f}, Energy: {energy_mean:.8f}, KL: {kl_div:.4e}, {cos_sim_str}, {i+1}/{n_test}, {t:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(x, y):\n",
    "    return 0.1 * x**2 + 8 * y**2\n",
    "\n",
    "\n",
    "energy_th, step = 3, 0.1\n",
    "x = np.arange(-energy_th, energy_th, step)\n",
    "y = np.arange(-energy_th, energy_th, step)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = loss_func(x, y)\n",
    "fig = plt.figure(num=1, figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(x, y, z, linewidth=0.5)\n",
    "\n",
    "\n",
    "def Optimizor(num_iter: int, learning_rate: float, momentum_coeff: float, label: str = None):\n",
    "    X, Y, Z = [], [], []\n",
    "    params = torch.tensor([2., 2.]).requires_grad_(True)\n",
    "    momentum = torch.zeros(params.shape)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        params.retain_grad()\n",
    "        loss = loss_func(*params)\n",
    "        loss.backward()\n",
    "        X.append(params[0].detach().numpy())\n",
    "        Y.append(params[1].detach().numpy())\n",
    "        Z.append(loss.detach().numpy())\n",
    "        if momentum_coeff != 0:\n",
    "            momentum = momentum_coeff * momentum + params.grad\n",
    "            params.grad = momentum\n",
    "        params = params - learning_rate * params.grad\n",
    "\n",
    "    ax.plot(X, Y, Z, linewidth=1.5, label=label)\n",
    "    ax.scatter(X, Y, Z, linewidth=0.5)\n",
    "\n",
    "\n",
    "n_test = 50\n",
    "learning_rate = 0.1\n",
    "Optimizor(n_test, learning_rate, momentum_coeff=0.0, label='no momentum')\n",
    "Optimizor(n_test, learning_rate, momentum_coeff=0.7, label='momentum=0.7')\n",
    "\n",
    "ax.view_init(elev=60, azim=0)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from typing import List\n",
    "from utils import fidelity\n",
    "from scipy.io import loadmat\n",
    "from VAE_model import VAEModel\n",
    "from itertools import combinations\n",
    "import torch.distributions as dists\n",
    "from torch.utils.data import DataLoader\n",
    "from qutrit_synthesis import NUM_PR, two_qutrit_unitary_synthesis\n",
    "\n",
    "np.set_printoptions(precision=15, linewidth=200)\n",
    "torch.set_printoptions(precision=15, linewidth=200)\n",
    "\n",
    "n_test = 1\n",
    "name = 'VGON_nqd7_20241024_185140'\n",
    "if True:\n",
    "    match = loadmat(f'./mats/{name}.mat')\n",
    "    n_qudits = match['n_qudits'].item()\n",
    "    n_qubits = match['n_qubits'].item()\n",
    "    batch_size = match['batch_size'].item()\n",
    "\n",
    "    n_layers = 2\n",
    "    beta = -1 / 3\n",
    "    n_samples = batch_size * n_test\n",
    "    n_params = n_layers * (n_qudits - 1) * NUM_PR\n",
    "\n",
    "    z_dim = 50\n",
    "    list_z = np.arange(np.floor(np.log2(n_params)), np.ceil(np.log2(z_dim)) - 1, -1)\n",
    "    h_dim = np.power(2, list_z).astype(int)\n",
    "\n",
    "    dev = qml.device('default.qubit', n_qubits)\n",
    "    gpu_memory = gpus[0].memoryUtil if (gpus := GPUtil.getGPUs()) else 1\n",
    "    if torch.cuda.is_available() and gpu_memory < 0.8 and n_qubits >= 12:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    def spin_operator(obj: List[int]):\n",
    "        if len(obj) != 2:\n",
    "            raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "        sx = qml.X(obj[0]) + qml.X(obj[1])\n",
    "        sy = qml.Y(obj[0]) + qml.Y(obj[1])\n",
    "        sz = qml.Z(obj[0]) + qml.Z(obj[1])\n",
    "        return sx, sy, sz\n",
    "\n",
    "    def spin_operator2(obj: List[int]):\n",
    "        if len(obj) != 2:\n",
    "            raise ValueError(f'The number of object qubits {len(obj)} should be 2')\n",
    "        s1 = spin_operator(obj)\n",
    "        s2 = [i @ j for i in s1 for j in s1]\n",
    "        return s2\n",
    "\n",
    "    def Hamiltonian(n_qudits: int, beta: float):\n",
    "        ham1, ham2 = 0, 0\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj1 = [2 * i, 2 * i + 1]\n",
    "            obj2 = [2 * i + 2, 2 * i + 3]\n",
    "            ham1 += qml.sum(*[spin_operator(obj1)[i] @ spin_operator(obj2)[i] for i in range(3)])\n",
    "            ham2 += qml.sum(*[spin_operator2(obj1)[i] @ spin_operator2(obj2)[i] for i in range(9)])\n",
    "        ham = ham1 / 4 - beta * ham2 / 16\n",
    "        coeffs, obs = qml.simplify(ham).terms()\n",
    "        coeffs = torch.tensor(coeffs).real\n",
    "        return qml.Hamiltonian(coeffs, obs)\n",
    "\n",
    "    def qutrit_symmetric_ansatz(params: torch.Tensor):\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj = list(range(n_qubits - 2 * i - 4, n_qubits - 2 * i))\n",
    "            two_qutrit_unitary_synthesis(params[i], obj)\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_state(n_layers: int, params: torch.Tensor):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.state()\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_expval(n_layers: int, params: torch.Tensor, Ham):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.expval(Ham)\n",
    "\n",
    "    state_dict = torch.load(f'./mats/{name}.pt', map_location=device, weights_only=True)\n",
    "    model = VAEModel(n_params, z_dim, h_dim).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    data_dist = dists.Uniform(0, 1).sample([n_samples, n_params])\n",
    "    test_data = DataLoader(data_dist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    Ham = Hamiltonian(n_qudits, beta)\n",
    "for i, batch in enumerate(test_data):\n",
    "    with torch.no_grad():\n",
    "        param, _, _ = model(batch.to(device))\n",
    "\n",
    "    cos_sims = torch.empty((0), device=device)\n",
    "    for ind in combinations(range(batch_size), 2):\n",
    "        sim = torch.cosine_similarity(param[ind[0], :], param[ind[1], :], dim=0)\n",
    "        cos_sims = torch.cat((cos_sims, sim.unsqueeze(0)), dim=0)\n",
    "    print(f'Cos_Sim: {cos_sims.max():.12f}, {cos_sims.mean():.12f}, {cos_sims.min():.12f}')\n",
    "\n",
    "    state = circuit_state(n_layers, param)\n",
    "    fidelities = torch.empty((0), device=device)\n",
    "    for ind in combinations(range(batch_size), 2):\n",
    "        fidelity = qml.math.fidelity_statevector(state[ind[0]], state[ind[1]])\n",
    "        fidelities = torch.cat((fidelities, fidelity.unsqueeze(0)), dim=0)\n",
    "    print(f'Fidelity: {fidelities.max():.12f}, {fidelities.mean():.12f}, {fidelities.min():.12f}')\n",
    "\n",
    "    energy = circuit_expval(n_layers, param, Ham)\n",
    "    print(f'Energy: {energy.max():.12f}, {energy.mean():.12f}, {energy.min():.12f}')\n",
    "\n",
    "    for i, ind in enumerate(combinations(range(batch_size), 2)):\n",
    "        print(f'{ind}: Cos_Sim: {cos_sims[i]:.12f}, Fidelity: {fidelities[i]:.12f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from typing import List\n",
    "from VAE_model import VAEModel\n",
    "from itertools import combinations\n",
    "import torch.distributions as dists\n",
    "from torch.utils.data import DataLoader\n",
    "from qutrit_synthesis import NUM_PR, two_qutrit_unitary_synthesis\n",
    "\n",
    "np.set_printoptions(precision=15, linewidth=200)\n",
    "torch.set_printoptions(precision=15, linewidth=200)\n",
    "\n",
    "n_test = 100\n",
    "n_qudits = 4\n",
    "if True:\n",
    "    n_layers = 2\n",
    "    batch_size = 1\n",
    "    n_qubits = 2 * n_qudits\n",
    "    n_samples = batch_size * n_test\n",
    "    n_params = n_layers * (n_qudits - 1) * NUM_PR\n",
    "\n",
    "    z_dim = 50\n",
    "    list_z = np.arange(np.floor(np.log2(n_params)), np.ceil(np.log2(z_dim)) - 1, -1)\n",
    "    h_dim = np.power(2, list_z).astype(int)\n",
    "\n",
    "    dev = qml.device('default.qubit', n_qubits)\n",
    "    gpu_memory = gpus[0].memoryUtil if (gpus := GPUtil.getGPUs()) else 1\n",
    "    if torch.cuda.is_available() and gpu_memory < 0.8 and n_qubits >= 12:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(f'PyTorch Device: {device}')\n",
    "\n",
    "    def qutrit_symmetric_ansatz(params: torch.Tensor):\n",
    "        for i in range(n_qudits - 1):\n",
    "            obj = list(range(n_qubits - 2 * i - 4, n_qubits - 2 * i))\n",
    "            two_qutrit_unitary_synthesis(params[i], obj)\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_state_init(n_layers: int, params: torch.Tensor):\n",
    "        params = params.reshape(n_layers, n_qudits - 1, NUM_PR)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.state()\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='best')\n",
    "    def circuit_state(n_layers: int, params: torch.Tensor):\n",
    "        params = params.transpose(0, 1).reshape(n_layers, n_qudits - 1, NUM_PR, batch_size)\n",
    "        qml.layer(qutrit_symmetric_ansatz, n_layers, params)\n",
    "        return qml.state()\n",
    "\n",
    "    param_init = torch.Tensor(np.random.uniform(0, 1, n_params))\n",
    "    state_init = circuit_state_init(n_layers, param_init).to(device)\n",
    "\n",
    "    model = VAEModel(n_params, z_dim, h_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    data_dist = dists.Uniform(0, 1).sample([n_samples, n_params])\n",
    "    train_data = DataLoader(data_dist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    params = torch.empty((0), device=device)\n",
    "    states = torch.empty((0), device=device)\n",
    "    fidelities = torch.empty((0), device=device)\n",
    "for i, batch in enumerate(train_data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    param, _, _ = model(batch.to(device))\n",
    "    params = torch.cat((params, param), dim=0)\n",
    "\n",
    "    state = circuit_state(n_layers, param)\n",
    "    states = torch.cat((states, state), dim=0)\n",
    "\n",
    "    fidelity = qml.math.fidelity_statevector(state_init, state)\n",
    "    fidelities = torch.cat((fidelities, fidelity), dim=0)\n",
    "\n",
    "    loss = fidelity.squeeze(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        t = time.perf_counter() - start\n",
    "        print(f'Loss: {loss:.15f}, {loss:.8e}, {i+1}/{n_test}, {t:.2f}')\n",
    "f_sort, f_ind = fidelities.sort()\n",
    "f_sort, f_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div(p, q):\n",
    "    p = torch.softmax(p, dim=0)\n",
    "    q = torch.softmax(q, dim=0)\n",
    "    return (p * (p.log2() - q.log2())).sum().unsqueeze(0)\n",
    "\n",
    "\n",
    "def JS_div(p, q):\n",
    "    p = torch.softmax(p, dim=0)\n",
    "    q = torch.softmax(q, dim=0)\n",
    "    return (p * p.log2() / 2 + q * q.log2() / 2 - (p + q) * ((p + q) / 2).log2() / 2).sum().unsqueeze(0)\n",
    "\n",
    "\n",
    "def H_distance(p, q):\n",
    "    p = torch.softmax(p, dim=0)\n",
    "    q = torch.softmax(q, dim=0)\n",
    "    return ((p.sqrt() - q.sqrt()).pow(2).sum() / 2).sqrt().unsqueeze(0)\n",
    "\n",
    "\n",
    "KL_divs = torch.empty((0), device=device)\n",
    "JS_divs = torch.empty((0), device=device)\n",
    "cos_sims = torch.empty((0), device=device)\n",
    "H_dist = torch.empty((0), device=device)\n",
    "for i in range(n_test):\n",
    "    cos_sim = torch.cosine_similarity(param_init, params[i], dim=0)\n",
    "    cos_sims = torch.cat((cos_sims, cos_sim.unsqueeze(0)), dim=0)\n",
    "    KL_divs = torch.cat((KL_divs, KL_div(param_init, params[i])), dim=0)\n",
    "    JS_divs = torch.cat((JS_divs, JS_div(param_init, params[i])), dim=0)\n",
    "    H_dist = torch.cat((H_dist, H_distance(param_init, params[i])), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot(x: torch.Tensor, label: str):\n",
    "    plt.plot(np.arange(n_test), x.detach().numpy(), label=label)\n",
    "\n",
    "\n",
    "plot(f_sort, 'Fidelity')\n",
    "plot(cos_sims[f_ind], 'Cos_Sim')\n",
    "plot(KL_divs[f_ind], 'KL_div')\n",
    "plot(JS_divs[f_ind], 'JS_div')\n",
    "plot(H_dist[f_ind], 'H_dist')\n",
    "plt.xticks(np.linspace(0, n_test, 11))\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = -1 / 3\n",
    "n_qudits = 7\n",
    "n_qubits = 2 * n_qudits\n",
    "t1 = time.perf_counter()\n",
    "ham = qutrit_AKLT_model(n_qudits, beta)\n",
    "t2 = time.perf_counter()\n",
    "print('Time:', t2 - t1)\n",
    "eigvals, eigvecs = eigsh(ham, k=4, which='SA')\n",
    "# eigvals, eigvecs = eigsh(ham, k=4, sigma=-4)\n",
    "t3 = time.perf_counter()\n",
    "print('Time:', t3 - t2)\n",
    "eigvecs = orth(eigvecs)\n",
    "t4 = time.perf_counter()\n",
    "print('Time:', t4 - t3)\n",
    "\n",
    "for i in range(len(eigvals)):\n",
    "    print(np.allclose(ham @ eigvecs[:, i], eigvals[i] * eigvecs[:, i], atol=5e-14), norm(eigvecs[:, i], 2), eigvals)\n",
    "for i in combinations(range(len(eigvals)), 2):\n",
    "    print(i, fidelity(eigvecs[:, i[0]], eigvecs[:, i[1]]))\n",
    "t5 = time.perf_counter()\n",
    "print('Time:', t5 - t4)\n",
    "\n",
    "ED_states = loadmat('./mats/ED_degeneracy.mat')[f'nqd{n_qudits}'][0, 1]\n",
    "\n",
    "print(np.count_nonzero(eigvecs), end=' ')\n",
    "eigvecs[np.abs(eigvecs) < 1e-15] = 0\n",
    "print(np.count_nonzero(eigvecs))\n",
    "print(eigvals, csr_matrix(eigvecs))\n",
    "\n",
    "mat_path = f'./mats/ED_degeneracy.mat'\n",
    "# updatemat(mat_path, {f'nqd{n_qudits}': (eigvals, eigvecs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qudits, beta = 7, -1 / 3\n",
    "Ham = AKLT_model(n_qudits, beta)\n",
    "h1 = csr_matrix(Ham.matrix())\n",
    "h2 = qubit_AKLT_model(n_qudits, beta)\n",
    "h3 = qutrit_AKLT_model(n_qudits, beta)\n",
    "\n",
    "v1 = np.sort(eigsh(h1, k=6, which='SA', return_eigenvectors=False))\n",
    "print(v1)\n",
    "v2 = np.sort(eigsh(h2, k=6, which='SA', return_eigenvectors=False))\n",
    "print(v2)\n",
    "v3 = np.sort(eigsh(h3, k=6, which='SA', return_eigenvectors=False))\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "print('L', end=' 　')\n",
    "beta_list = [-0.3, -0.2, -0.1, 0.0, 0.4]\n",
    "for beta in beta_list:\n",
    "    if beta < 0:\n",
    "        print(f'β = {beta:.2f}', end='　')\n",
    "    else:\n",
    "        print(f'β = +{beta:.2f}', end='　')\n",
    "print('Time')\n",
    "for n_qudits in [4, 5, 6, 7, 8, 9]:\n",
    "    start_nq = time.perf_counter()\n",
    "    print(n_qudits, end=' 　')\n",
    "    s1 = qubit_spin_operator(n_qudits)\n",
    "    s2 = qubit_spin_operator2(n_qudits)\n",
    "    for beta in beta_list:\n",
    "        ham = s1 - beta * s2\n",
    "        eigvals = eigsh(ham, k=4, which='SA', return_eigenvectors=False)\n",
    "        eigvals = sorted(eigvals)\n",
    "        vals = np.array(eigvals[:1])\n",
    "        for v1 in eigvals[1:]:\n",
    "            for v2 in vals:\n",
    "                if np.abs(v1 - v2) < 1e-12:\n",
    "                    break\n",
    "            else:\n",
    "                vals = np.append(vals, v1)\n",
    "        diff = vals[0] - vals[1]\n",
    "        print(f'{diff:.6f}', end='　')\n",
    "    end_nq = time.perf_counter()\n",
    "    print(f'{(end_nq-start_nq):.2f}')\n",
    "end = time.perf_counter()\n",
    "total = end - start\n",
    "if total >= 60:\n",
    "    print(f'Total time: {total//60:.0f}m{total%60:.2f}s')\n",
    "else:\n",
    "    print(f'Total time: {total:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx = {2: np.array([[0, 1], [1, 0]], dtype=CDTYPE) / 2,  \\\n",
    "      3: np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=CDTYPE) / np.sqrt(2)}\n",
    "Sy = {2: np.array([[0, -1j], [1j, 0]], dtype=CDTYPE) / 2, \\\n",
    "      3: np.array([[0, -1j, 0], [1j, 0, -1j], [0, 1j, 0]], dtype=CDTYPE) / np.sqrt(2)}\n",
    "Sz = {2: np.array([[1, 0], [0, -1]], dtype=CDTYPE) / 2, \\\n",
    "      3: np.array([[1, 0, 0], [0, 0, 0], [0, 0, -1]], dtype=CDTYPE)}\n",
    "\n",
    "dim = 3\n",
    "print(Sx[dim])\n",
    "print(Sy[dim])\n",
    "print(Sz[dim])\n",
    "\n",
    "S = Sx[dim] + Sy[dim] + Sz[dim]\n",
    "print(S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
